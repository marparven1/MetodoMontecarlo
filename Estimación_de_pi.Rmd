---
title: "Estimación de \\(\\pi\\)"
output: html_document
---

```{r replicabilidad, include=FALSE}
set.seed(356764)
```

### Estimación de probabilidades

Nota: Este método falla cuando nos vamos a los extremos, muy pocos elementos generados perteneces a A o muchos elementos generados pertenecen a A.


Sean \( \mathbf{X} \) un vector aleatorio que sigue una distribución de probabilidad con función de densidad \( f \) y \( A \subseteq \mathbb{R}^{d} \) un conjunto de Borel y consideremos el problema de estimar \( p = \mathbb{P}(\mathbf{X} \in A) \). Si definimos la _función indicador_ \( 𝟙_{A} \) como
\begin{equation*}
  𝟙_{A}(\mathbf{x}) =
  \begin{cases}
    1 & \text{si } \mathbf{x} \in A, \\
    0 & \text{si } \mathbf{x} \not\in A.
  \end{cases}
\end{equation*}
entonces se tiene
\begin{equation*}
  \mathbb{P}(\mathbf{X} \in A) =
  \int_{A} f(\mathbf{x}) \mathop{}\!d \mathbf{x} =
  \int_{\mathbb{R}^{d}} 𝟙_{A}(\mathbf{x}) f(\mathbf{x}) \mathop{}\!d \mathbf{x} =
  \mathbb{E}[𝟙_{A}(\mathbf{X})]
\end{equation*}
y podemos aplicar el método de Montecarlo para obtener la estimación.

Por tanto, basta generar \( n \) valores independientes \( \mathbf{x}_{1}, \dotsc, \mathbf{x}_{n} \sim f \) y calcular
\begin{equation*}
  \hat{p}_{n} =
  \frac{1}{n} \sum_{i = 1}^{n} 𝟙_{A}(\mathbf{x}_{i}) =
  \frac{n_{A}}{n}
\end{equation*}
donde \( n_{A} \) es la cantidad de valores generados que pertenecen al conjunto \( A \).

Para construir un intervalo de confianza a partir del teorema central del límite necesitamos calcular el error estándar de \( \hat{p}_{n} \):
\begin{equation*}
  \mathrm{Var}(\hat{p}_{n}) =
  \frac{1}{n^{2}} \sum_{i = 1}^{n} \mathrm{Var}\bigl(𝟙_{A}(\mathbf{X})\bigr) =
  \frac{\mathrm{Var}\bigl(𝟙_{A}(\mathbf{X})\bigr)}{n} =
  \frac{\mathbb{E}\bigl[𝟙_{A}(\mathbf{X})^{2}\bigr] - \mathbb{E}\bigl[𝟙_{A}(\mathbf{X})\bigr]^{2}}{n} =
  \frac{p - p^{2}}{n} =
  \frac{p (1 - p)}{n}
\end{equation*}
ya que \( 𝟙_{A}(\mathbf{X})^{2} = 𝟙_{A}(\mathbf{X}) \).

Por tanto, un intervalo de confianza con probabilidad de cobertura \( 1 - \alpha \) sería
\begin{equation*}
  \Bigl[
    \hat{p}_{n} -
    \sqrt{\frac{\hat{p}_{n} (1 - \hat{p}_{n})}{n}} z_{1 - \frac{\alpha}{2}},
    \hat{p}_{n} +
    \sqrt{\frac{\hat{p}_{n} (1 - \hat{p}_{n})}{n}} z_{1 - \frac{\alpha}{2}},
  \Bigr]
\end{equation*}

Esta manera de obtener un intervalo de confianza se conoce como el método de Wald y proporciona usualmente intervalos con una probabilidad de cobertura menor a la requerida.

Una manera de obtener mejores intervalos de confianza, en el sentido de que la probabilidad de cobertura es más cercana a la requerida, es el método de Agresti-Coull. Este consiste en añadir unas pseudocuentas \( n_{0} \) y \( n_{1} \) de valores fuera y dentro, respectivamente, del conjunto \( A \). Estas pseudocuentas únicamente dependen del nivel de confianza requerido, en concreto \( n_{0} = n_{1} = z_{1 - \frac{\alpha}{2}}^{2}/2 \). De esta forma,
\begin{align*}
  \tilde{n} &= n + n_{0} + n_{1} \\
  \tilde{p}_{n} &= \frac{n_{A} + n_{1}}{\tilde{n}}
\end{align*}
y un intervalo de confianza con probabilidad de cobertura \( 1 - \alpha \) sería
\begin{equation*}
  \Bigl[
    \tilde{p}_{n} -
    \sqrt{\frac{\tilde{p}_{n} (1 - \tilde{p}_{n})}{\tilde{n}}}
    z_{1 - \frac{\alpha}{2}},
    \tilde{p}_{n} +
    \sqrt{\frac{\tilde{p}_{n} (1 - \tilde{p}_{n})}{\tilde{n}}}
    z_{1 - \frac{\alpha}{2}},
  \Bigr]
\end{equation*}

Fijado el valor \( n_{A} \) y usando que entonces la función de distribución de la binomial considerada como función de la probabilidad de éxito \( p \) es continua y decreciente, una forma de obtener un intervalo de confianza conservativo que asegure al menos el nivel de confianza \(1 - \alpha \) requerido es considerar el intervalo \( (p_{I}, p_{D}) \) tal que
\begin{equation*}
  \mathbb{P}_{\mathrm{binom}(n_{A}, p_{I})} \Bigl(\sum_{i = 1}^{n} 𝟙_{A}(\mathbf{x}_{i}) \geq n_{A}\Bigr) =
  \frac{\alpha}{2}
  \quad\text{y}\quad
  \mathbb{P}_{\mathrm{binom}(n_{A}, p_{D})} \Bigl(\sum_{i = 1}^{n} 𝟙_{A}(\mathbf{x}_{i}) \leq n_{A}\Bigr) =
  \frac{\alpha}{2}
\end{equation*}

Teniendo en cuenta la relación entre la distribución binomial y la distribución beta, se tiene que
\begin{equation*}
  p_{I} = F_{\mathrm{beta}(n_{A}, n - n_{A} + 1)}^{-1} \Bigl(\frac{\alpha}{2}\Bigr)
  \quad\text{y}\quad
  p_{D} = F_{\mathrm{beta}(n_{A} + 1, n - n_{A})}^{-1} \Bigl(1 - \frac{\alpha}{2}\Bigr)
\end{equation*}


### Aplicación a la estimación de \( \pi \)

Vamos a aplicar lo anterior para realizar una estimación del valor del número \( \pi \). Para ello, consideremos el vector aleatorio \( \mathbf{U} = (X, Y) \sim \mathrm{U}\bigl((0, 1)^{2}\bigr) \) y el conjunto
\begin{equation*}
  A = \{(x, y) \in (0, 1)^{2} \mid x^{2} + y^{2} \leq 1\}
\end{equation*}
que se puede visualizar en el siguiente gráfico:

```{r sector-circular, echo=FALSE}
library(ggplot2)

ggplot() +
  stat_function(
    geom = "area",
    fun = function(x) {
      sqrt(1 - x^2)
    },
    fill = "gray"
  ) +
  xlim(0, 1) +
  annotate("text",
    x = 0.375, y = 0.375, size = 25,
    parse = TRUE, label = "italic(A)"
  ) +
  coord_fixed()
```


Entonces,
\begin{equation*}
  \mathbb{P}\bigl((X, Y) \in A\bigr) =
  \int_{A} 1 \mathop{}\!d x \mathop{}\!d y =
  \frac{\pi}{4}
\end{equation*}
y podemos estimar su valor mediante el método de Montecarlo.

Empezamos estableciendo la forma de generar valores del vector aleatorio y definiendo la función \(g\) que se aplicará a esos valores, que en este caso se trata de la función indicador del conjunto \( A \).

```{r generacion}
genera_vector_aleatorio <- function() {
  runif(2)
}

g <- function(u) {
  x <- u[1]
  y <- u[2]
  x^2 + y^2 <= 1
}

# i(x^2+y^2 <=1){1}else{0}
```

Nótese que, tal y como la hemos definido, la función indicador devuelve `TRUE` o `FALSE`. No obstante, al realizar posteriormente operaciones aritméticas _R_  transformará esos valores a `1` y `0`, respectivamente, por lo que no habrá ningún problema.

Ahora basta replicar la generación de valores aleatorios uniformes en el cuadrado \((0, 1) \times (0, 1)\) y contar cuántos pertenecen al conjunto \( A \).

```{r replicacion}
n <- 1e3
valores_g <- replicate(n, {
  u <- genera_vector_aleatorio()
  g(u)
})

## repetir n veces negera_vector_aleatorio y luego le aplico g
```

Finalmente, estimamos el valor de \( \pi / 4 \) a partir de los valores generados y construimos intervalos de confianza según el método general de Montecarlo y según los tres métodos específicos para la estimación de probabilidades, todos con la misma probabilidad de cobertura para que tenga sentido compararlos.

```{r estimacion}
# La mejor manera de contar en R cuántos objetos verifican una propiedad es determinar para cada uno de ellos si cumple o no la propiedad y sumar los resultados.
n_A <- sum(valores_g) # me cuenta cuantos TRUE

probabilidad_cobertura <- 0.95 # Todos los IC tienen que ser con la misma prob de cobertura, sino no tiene sentido compararlos
alfa <- 1 - probabilidad_cobertura
percentil <- qnorm(1 - alfa / 2)

# Calculamos un intervalo de confianza según el método de Montecarlo
estimacion_Montecarlo <- n_A / n
error_estandar_Montecarlo <- sqrt(var(valores_g) / n)
intervalo_Montecarlo <-
  estimacion_Montecarlo + c(-1, 1) * error_estandar_Montecarlo * percentil

# Calculamos un intervalo de confianza según el método de Wald
estimacion_Wald <- n_A / n
error_estandar_Wald <- sqrt(estimacion_Wald * (1 - estimacion_Wald) / n)
intervalo_Wald <-
  estimacion_Wald + c(-1, 1) * error_estandar_Wald * percentil

# Calculamos un intervalo de confianza según el método de Agresti-Coull
n_0 <- n_1 <- percentil^2 / 2
n_Agresti_Coull <- (n + n_0 + n_1)
estimacion_Agresti_Coull <- (n_A + n_1) / n_Agresti_Coull
error_estandar_Agresti_Coull <-
  sqrt(estimacion_Agresti_Coull * (1 - estimacion_Agresti_Coull) /
    n_Agresti_Coull)
intervalo_Agresti_Coull <-
  estimacion_Agresti_Coull + c(-1, 1) * error_estandar_Agresti_Coull * percentil

# Calculamos un intervalo de confianza según el método conservativo
p_I <- qbeta(alfa / 2, shape1 = n_A, shape2 = n - n_A + 1)
p_D <- qbeta(1 - alfa / 2, shape1 = n_A + 1, shape2 = n - n_A)
estimacion_conservativo <- n_A / n
intervalo_conservativo <- c(p_I, p_D)
```

Teniendo en cuenta que el valor que se ha estimado es \( \pi / 4\), para obtener una estimación de \( \pi \) hay que multiplicar por \( 4 \) tanto las estimaciones como los intervalos de confianza obtenidos. Se tiene entonces lo siguiente:

```{r}
estimaciones <- 4 * c(
  estimacion_Montecarlo,
  estimacion_Wald,
  estimacion_Agresti_Coull,
  estimacion_conservativo
)
errores <- 4 * c(
  error_estandar_Montecarlo,
  error_estandar_Wald,
  error_estandar_Agresti_Coull,
  NA
)
intervalos <- 4 * rbind(
  intervalo_Montecarlo,
  intervalo_Wald,
  intervalo_Agresti_Coull,
  intervalo_conservativo
)
knitr::kable(
  data.frame(
    Intervalo = c("Montecarlo", "Wald", "Agresti-Coull", "Conservativo"),
    Estimación = estimaciones,
    `Error estándar` = errores,
    Intervalo = intervalos,
    Longitud = apply(intervalos, 1, diff)
  ),
  row.names = FALSE,
  digits = 10
)
```
